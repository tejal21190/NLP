{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejal21190/NLP/blob/main/Webscraping_and_Textual_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project of Webscrapping Texual analysis given by one of the company (Asked to keep their data confidential hence sharing only my work only ) "
      ],
      "metadata": {
        "id": "DM2F0tIoS5lF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGEVPDU_aBm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a9b64c5-8fef-479a-c1c8-e7c3b6e3f583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.9/dist-packages (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import sys\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "from googleapiclient.discovery import build\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRhab62WZHOp"
      },
      "source": [
        "# **File Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwn58uwjWmj1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_file_id(url):\n",
        "    file_id = None\n",
        "    if \"/file/d/\" in url:\n",
        "        file_id = url.split(\"/file/d/\")[1].split(\"/\")[0]\n",
        "    elif \"id=\" in url:\n",
        "        file_id = url.split(\"id=\")[1].split(\"&\")[0]\n",
        "    return file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfFE0EoWWxPx"
      },
      "outputs": [],
      "source": [
        "def get_words(list_urls):\n",
        "    list_of_words = []\n",
        "    for url in list_urls:\n",
        "        # extract the file ID from the URL\n",
        "        file_id = get_file_id(url)\n",
        "        # download the file\n",
        "        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "        response = requests.get(download_url)\n",
        "        data = response.text\n",
        "        lines = data.split(\"\\n\")\n",
        "        for line in lines:\n",
        "            word = line.split(\"|\")[0].replace(\"\\r\", \"\")\n",
        "            list_of_words.append(word)\n",
        "    return list_of_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPBUVUiIqm9T"
      },
      "outputs": [],
      "source": [
        "def get_url_df(url):\n",
        " \n",
        "    # Read data from a URL and get it as a pandas DataFrame.\n",
        "    \n",
        "    file_id = url.split('/')[-2]\n",
        "    df = pd.read_excel(f\"https://drive.google.com/uc?id={file_id}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article(url):\n",
        "  response = requests.get(url)\n",
        "\n",
        "  if response.status_code == 404:\n",
        "    article=''\n",
        "  else:\n",
        "    # Extract the title\n",
        "    title = BeautifulSoup(response.content, 'html.parser').title.text.strip().split(' | ')[0]  \n",
        "    # Extract article text\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Remove last line of author and college\n",
        "    pre_data = soup.find_all('pre')\n",
        "    for pre in pre_data:\n",
        "        pre.extract()\n",
        "\n",
        "    text_data = soup.find('div', class_='td-post-content')\n",
        "    text = text_data.get_text().strip()\n",
        "    article=title + '\\n\\n' + text\n",
        "    return article"
      ],
      "metadata": {
        "id": "-rd6WXgays4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56e97KYxZB3P"
      },
      "source": [
        "# **Textual Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXbn5Io3Y9dt"
      },
      "outputs": [],
      "source": [
        "def syllable_count(word):\n",
        "    vowels = 'aeiouy'\n",
        "    count = 0\n",
        "    if word.endswith(('ed', 'es')):\n",
        "        return count\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "            if word.endswith('e'):\n",
        "                count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCJ9IS9taLDH"
      },
      "outputs": [],
      "source": [
        "class WebScrap():\n",
        "\n",
        "    def __init__(self):\n",
        "        # URLs for all files\n",
        "        file_url = \"https://docs.google.com/spreadsheets/d/\" ## Add google drive link of spreadsheet containing links of webpages which need to be webscrapped\n",
        "        pos_url = [# Add txt file google drive link containing list of positive words]\n",
        "        neg_url = [# Add txt file google drive link containing list of negative words]\n",
        "        stop_urls = [# Add txt file google drive link containing list of stop words]\n",
        "        # Try to fetch data from the URLs\n",
        "        try:\n",
        "          self.url_df = get_url_df(file_url)\n",
        "          self.positive_words = get_words(pos_url)\n",
        "          self.negative_words = get_words(neg_url)\n",
        "          self.stop_words = get_words(stop_urls)\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error occurred while fetching data from links\")\n",
        "          raise e\n",
        "  # Claculate all scores\n",
        "    def all_score(self, text, url,id):\n",
        "         # tokenization of text\n",
        "        tokens = [token for token in word_tokenize(text.lower()) if\n",
        "                  token not in self.stop_words and token not in string.punctuation]\n",
        "        # Positive and Negative score\n",
        "        if tokens:\n",
        "            pos_score = sum([1 for token in tokens if token in self.positive_words])\n",
        "            neg_score = sum([1 for token in tokens if token in self.negative_words])\n",
        "\n",
        "            # Calculate polarity score\n",
        "            polarity_score = (pos_score - neg_score) / len(tokens)\n",
        "\n",
        "            # Calculate subjectivity scores\n",
        "            total_words = len(tokens)\n",
        "            subjective_score = (pos_score + neg_score) / (total_words + 0.000001)\n",
        "\n",
        "            # average sentence length\n",
        "            sentences = sent_tokenize(text)\n",
        "            number_of_sentences = len(sentences)\n",
        "            # Word Count\n",
        "            number_of_words = total_words\n",
        "            avg_sentence_length = number_of_words / number_of_sentences\n",
        "            \n",
        "            # Complex Word Count for more than 2 syllable\n",
        "            number_of_complex_words = len([word for word in tokens if syllable_count(word) > 2])\n",
        "            percentage_of_complex_words = (number_of_complex_words / number_of_words) * 100\n",
        "\n",
        "            # Calculate fog index\n",
        "            fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words) if number_of_words > 0 else 0\n",
        "\n",
        "            # Average Number of Words Per Sentence\n",
        "            Avg_number_words_per_sentence= number_of_words / number_of_sentences\n",
        "\n",
        "            # Syllable Count Per Word\n",
        "            syllable_dict = {}\n",
        "            for word in tokens:\n",
        "                syllable_dict[word] = syllable_dict.get(word, 0) + syllable_count(word)\n",
        "            Syllable_count_per_word=syllable_dict    \n",
        "\n",
        "            # Personal Pronouns\n",
        "            pronouns = ['i', 'we', 'my', 'ours', 'us']\n",
        "            # Regex pattern for not \"US\"\n",
        "            pattern = r'\\b(?!US\\b)[a-zA-Z]+\\b'\n",
        "            tokens = re.findall(pattern, text.lower())\n",
        "            pronoun_count = sum([1 for token in tokens if token in pronouns])\n",
        "            \n",
        "            # Average Word Length\n",
        "            total_chars = sum(len(word) for word in tokens)\n",
        "            Avg_word_length = total_chars / total_words\n",
        "\n",
        "        else:\n",
        "            pos_score = 0\n",
        "            neg_score = 0\n",
        "            polarity_score = 0\n",
        "            subjective_score = 0\n",
        "            number_of_sentences = 0\n",
        "            number_of_words = 0\n",
        "            number_of_complex_words = 0\n",
        "            avg_sentence_length = 0\n",
        "            percentage_of_complex_words = 0\n",
        "            fog_index = 0\n",
        "            Avg_number_words_per_sentence=0\n",
        "            pronoun_count = 0\n",
        "            Syllable_count_per_word={}\n",
        "            Avg_word_length=0\n",
        "\n",
        "        # Return scores as a dictionary\n",
        "        scores = {\n",
        "            'URL_ID':id,\n",
        "            \"URL\" : url,\n",
        "            'POSITIVE SCORE': pos_score,\n",
        "            'NEGATIVE SCORE': neg_score,\n",
        "            'POLARITY SCORE': polarity_score,\n",
        "            'SUBJECTIVITY SCORE': subjective_score,\n",
        "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "            'PERCENTAGE OF COMPLEX WORDS': percentage_of_complex_words,\n",
        "            'FOG INDEX': fog_index,\n",
        "            'AVG NUMBER OF WORDS PER SENTENCE': Avg_number_words_per_sentence,\n",
        "            'COMPLEX WORD COUNT':number_of_complex_words,\n",
        "            'WORD COUNT':number_of_words,\n",
        "            'SYLLABLE PER WORD':Syllable_count_per_word,\n",
        "            'PERSONAL PRONOUNS': pronoun_count,\n",
        "            'AVG WORD LENGTH':Avg_word_length\n",
        "        }\n",
        "        return scores\n",
        "\n",
        "    def start_scraping(self):\n",
        "        results = []\n",
        "       \n",
        "        for index, row in self.url_df.iterrows():\n",
        "            url = (row[\"URL\"])\n",
        "            id=(row[\"URL_ID\"])\n",
        "            article_text = extract_article(url)\n",
        "            # if article_text is not None:\n",
        "            #     result = self.all_score(text=article_text, url = url,id=id)\n",
        "            #     result[\"URL\"] = url\n",
        "            #     results.append(result)\n",
        "            #     print(result)\n",
        "            # else:\n",
        "            if article_text is None:\n",
        "              article_text=\"\"\n",
        "            result = self.all_score(text=article_text, url = url,id=id)\n",
        "            result[\"URL\"] = url\n",
        "            results.append(result)\n",
        "            print(result)\n",
        "            save_text_file(article_text,id)\n",
        "             \n",
        "        write_dict_to_xlsx(results,\"location\"+\"Output Data Structure.xlsx\") #location where the result must be saved\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj5acgk7Z7Cq"
      },
      "source": [
        "# **Save Output File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-wVMv5wZQXT"
      },
      "outputs": [],
      "source": [
        "def write_dict_to_xlsx(data, filename):\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # set the hyperlink format\n",
        "    writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
        "    url_format = writer.book.add_format({'font_color': 'blue', 'underline': 1})\n",
        "    \n",
        "    df.to_excel(writer, index=False)\n",
        "    \n",
        "    worksheet = writer.sheets['Sheet1']\n",
        "    url_col = list(df.columns).index('URL')\n",
        "    worksheet.set_column(url_col, url_col, 30, url_format)\n",
        "    writer.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOxfMrT1bDBj"
      },
      "source": [
        "# **Saving Article into text file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypf7Xe_ZXbjf"
      },
      "outputs": [],
      "source": [
        "def save_text_file(data,filename):\n",
        "  file=\"Location\"+str(filename)+\".txt\" #Location where to save txt file\n",
        "  a= open(file,'a+')\n",
        "  a.write(data)\n",
        "  a.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvLOG9VOa3KP"
      },
      "source": [
        "# **Start Scrapping and Textual Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8aePVg-a2k6"
      },
      "outputs": [],
      "source": [
        "w = WebScrap()\n",
        "w.start_scraping()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ADek67jZ0Yy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}